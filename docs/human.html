<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>human</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://chaishekar.github.io/Portfolio/">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./human.html#intro">
 <span class="menu-text">Introduction</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-analysis" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Analysis</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-analysis">    
        <li>
    <a class="dropdown-item" href="./human.html#analysis">
 <span class="dropdown-text">Exploratory Data Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./human.html#feature">
 <span class="dropdown-text">Feature Engineering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./human.html#model">
 <span class="dropdown-text">Modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./human.html#results">
 <span class="dropdown-text">Results</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./human.html#conclusion">
 <span class="menu-text">Conclusion</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chaishekar/HAR-Neural-Network-Project"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">



<div style="text-align: center;">
<p><b style="font-size: 35px; color: #d6aaa7;">Human Activity Recognition - Neutral Network and Deep Learning Project</b></p>
</div>
<section id="intro" class="level3">
<h3 class="anchored" data-anchor-id="intro">Introduction</h3>
<p>Humans have a natural ability to understand information conveyed through body movements, gestures, and postures. We can effortlessly track and interpret human motions, interactions, and even intentions. This complex recognition, processed almost subconsciously by our brain, is a result of the visual inputs we receive. Machines, in contrast, are still developing the ability to recognize human activities. We guide this learning process, drawing from our understanding of these tasks. Consider that just sixty years ago, computers were primarily used as simple calculators. Since then, advancements in Machine Learning (ML), a subset of Artificial Intelligence (AI), have significantly enhanced their ability to interpret and respond to various situations, mimicking human-like understanding. Research in Human Activity Recognition (HAR) began in the early 1980s, but the most substantial advancements have occurred in the last two decades. These breakthroughs can be attributed to improvements in microelectronics, sensor technology, and computer systems. These technological advances enable the collection of more detailed data from human movements.</p>
</section>
<section id="about-the-data" class="level3">
<h3 class="anchored" data-anchor-id="about-the-data">About the Data</h3>
<p>Amongst the most renowned HAR datasets, one is hosted by the University of California Irvine (UCI) in their Machine Learning Repository, which is commonly known as the UCI HAR dataset. Human Activity Recognition database is built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Sam- sung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="././images/fig20.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="analysis" class="level3">
<h3 class="anchored" data-anchor-id="analysis">Analysis</h3>
<p>The data is split into training and test sets where 70% of the volunteers were selected for generating the training data and 30% the test data. The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low-frequency components; therefore, a filter with 0.3 Hz cutoff frequency was used. This is already done and available at the UCI HAR dataset website. The preprocessed dataset contains 561 features, which are the result of the pre-processing of the raw data. The features are normalized and bounded within [-1,1]. Each feature vector is a row on the text file. This dataset will be used for our LSTM Model while the feature engineered raw data will be used for our CNN Model.</p>
<section id="exploratory-data-analysis" class="level4">
<h4 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory Data Analysis</h4>
<p>The raw dataset has feature vectors representing each window. We wanted to see how dif- ferent is the data across all the axes for different activities and also among the activities for consecutive time windows. 2 activities were chosen for this analysis, namely, Walking and Standing. The following plots show the distribution of the data across all the axes for these 2 activities.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig21.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig22.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The plots show that the data is very different across the axes and also across the time windows. This shows that the data is very dynamic and the features extracted from this data will be very useful in classifying the activities.</p>
</section>
<section id="feature" class="level4">
<h4 class="anchored" data-anchor-id="feature">Feature Engineering</h4>
<p>The t-SNE graph of our raw dataset shows the distribution of different classes on a two- dimensional plane. We observe that samples from four distinct classes are grouped closely together, which is not ideal for effective classification.</p>
<section id="frequency-features" class="level5">
<h5 class="anchored" data-anchor-id="frequency-features">Frequency Features</h5>
<p>Human actions can be represented as combinations of sinusoidal signals with varying frequen- cies. We refer to the frequency information as the values and amplitudes of these signals at different frequencies. To extract this information, we use the Fast Fourier Transform (FFT), an algorithm developed in 1965 by James Cooley and John Tukey. It’s a quicker version of the Discrete Cosine Transform (DFT) for calculating the frequency components of time-domain signals. Applying FFT to our dataset, the t-SNE graph shows improved clustering of similar classes and better separation of different classes, enhancing the ease of classification.</p>
</section>
<section id="power-features" class="level5">
<h5 class="anchored" data-anchor-id="power-features">Power Features</h5>
<p>The power spectrum of a signal shows how power is distributed across various frequencies. We use the pwelch algorithm for Welch’s power spectral density estimate, which calculates power using overlapping data segments and a windowing technique. This method, developed by P.D. Welch in 1967, enhances control over resolution and reduces variance in the estimation. The t-SNE graph of power-featured signals shows distinct class separations. Despite the distinctions in the t-SNE graphs based on frequency and power features, some overlap of different classes remains in both cases. Therefore, utilizing both sets of features could lead to more accurate identification of the unique properties of the samples.</p>
</section>
</section>
</section>
<section id="model" class="level3">
<h3 class="anchored" data-anchor-id="model">Modeling</h3>
<section id="long-short-term-memory-lstm" class="level4">
<h4 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h4>
<p>LSTM is a type of Recurrent Neural Network (RNN) that can learn long-term dependencies. It is a special kind of RNN, capable of learning long-term dependencies. It was introduced by Hochreiter and Schmidhuber in 1997. The dataset is primarily a time series dataset because each data point is a 2.56-second window containing 128 readings. We initially developed a basic LSTM model with 50 hidden units and included a 0.2 dropout rate to prevent overfitting. Although this model performed well, it didn’t yield the most promising results. To improve it, we applied GridSearch CV for hyperparameter tuning. This process helped us identify the optimal settings: 150 hidden units and a 0.3 dropout rate, using the Adam Optimizer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig23.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="two-channel-convolutional-neural-network-cnn" class="level4">
<h4 class="anchored" data-anchor-id="two-channel-convolutional-neural-network-cnn">Two channel Convolutional Neural Network (CNN)</h4>
<p>CNNs are a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, and natural language processing. Now the raw data was 3 axial data for Body acceleration, Body gyroscope and Total accelera- tion. It was noticed that since the data has 3 axes namely X, Y and Z which led our shape of our data to be (10300, 384, 3). Where 10000 is the number of timesteps, 384 is 128 readings per time step multiplied by 3 different readings for Body acceleration, Body gyroscope and Total acceleration and 3 is the channels. This data shape looks awfully similar to a RGB Image which also has 3 channels, 1 for each color.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig24.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Two CNNs (specified as channels) are incorporated in the classification model; one of them will process the frequency features, and the other will work with the power features. Both the CNNs have similar architectures with 2 1 dimensional Convolutional layers of 32 units each with 2x2 Maxpooling kernels and Batch normalization. Outputs from these were flattened, concatenated and passed through a Dense layer of 128 units with a dropout rate of 0.3 and finally the output layer with softmax activation to classify into 6 activities.</p>
</section>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>Both the models (LSTM and CNN) gave similar results with a high accuracy of 95% for LSTM and 96% for CNN. The training and testing accuracy and loss plots are shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig25.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/fig26.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><img src="./images/fig27-01.png" class="img-fluid"></p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>This project successfully demonstrated how deep learning can be effectively used in Human Activity Recognition (HAR), particularly through the use of LSTM and CNN models. Inter- estingly, despite their distinct architectural designs, both models achieved similar high levels of accuracy. This outcome highlights the adaptability of different machine learning approaches to complex data analysis. The study focused on the UCI HAR dataset, which comprises time-series data from smartphone sensors. The LSTM model performed very well due to its ability to process sequential data, making it well-suited for the time-dependent nature of the dataset. This was key to its high performance in recognizing patterns in human movements over time. Conversely, the CNN model leveraged the three-dimensional structure of the data, which is similar to RGB image formats, with readings from three axes. This resemblance allowed the CNN to effectively extract spatial features, leading to its impressive performance in activity classification. The project’s findings are significant as they show how different types of neural network models can be optimized to handle specific data characteristics — LSTM for time-series data and CNN for multi-dimensional data. This approach not only achieved accuracy levels above 95% but also provided valuable insights into the application of machine learning in HAR. These insights could have far-reaching implications in various fields, from healthcare to interactive technology. Overall, this research contributes to the evolving field of HAR, offering insights and tools that could be applied in various practical sc</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>